---
title: "**The Economics of App Success: How Revenue Streams Influence Downloads and User Ratings**"
subtitle: "Group Assignment - Strategy and Business Models"
author: "Group 5"
date: "November 28, 2024"
date-format: DD/MM/YYYY # Set date format to long
crossref:
  fig-prefix: Figure   # (default is "Figure")
  tbl-prefix: Table    # (default is "Table")
format: 
  pdf: #Output pdf
    documentclass: article
    fig-pos: "H"
    number-sections: true
    toc: true #table of contents
    toc-title: Contents #name table of contents
    papersize: a4
    fontsize: 12pt
    fontfamily: mathptmx
    geometry:
      - top=25mm #top margin
      - left=25mm #left margin
    pdf-engine: pdflatex
    cap-location: bottom # Set table caption location to bottom
    include-in-header: 
      - text: |
          \usepackage{graphicx}
    include-before-body:
      - text: |
          \vspace{2em}
          \begin{center}
            \includegraphics[width=0.3\textwidth]{figures/jads.png}
          \end{center}
          \vspace{3em}
          \newpage
bibliography: references.bib
bibliographystyle: apa
execute: 
  enabled: true
jupyter: python3
---

<!-- horizontal line -->

\newpage

# Introduction <!-- 800-1,000 words -->

```{=html}
<!-- Content: Introduce the topic and explain its relevance in both academic and practical contexts. Justify the research gap and formulate a clear and insightful research question.

Key Elements:
Background of the topic.
Identification of the research gap.
Relevance to current business practices.
Research question. -->
```

With the every-growing popularity of cellphones [@charted2023], the popularity of mobile applications is also steadily increasing. In 2024, mobile applications are estimated to generate over \$900 billion in revenue [@global2019]. Generally, mobile applications ('*apps*' from here on) tend to be categorized in three different categories [@roma2016]. Paid apps are the most transparent; they revenue is based on an up-front purchase by the user. Free apps, on the other hand, require no purchase by the user at any stage. According to @roma2016, these apps make their revenue from deals with third-parties, either through advertisement or other purposes such as market information.

Finally, freemium apps are, as the name suggests, a middle-ground between free and premium. Users get access to a basic version of the application first and can unlock more features through an in-app payment [@kumar2014]. Of these three revenue models, freemium is the most commonly used and the most [@salehudin2021] and leads to more downloads as well as revenue [@liu2014].

## Academic Background

<!-- Kort samengevat: Wat is de wetenschappelijke consensus over dit topic? Dit wordt verder uitgediept in de Theory and Hypothesis sectie.-->

Most research uses these three established categories—paid, freemium, and free—when discussing revenue models for apps. However, by limiting the discussion to these three terms, nuances within these categories might be missed.

In a review paper from 2023 [@djaruma2023], different levels of monetization are suggested based on previous literature. These levels provide a clear framework for the revenue models of mobile apps.

| Strategy | Description |
|------------------------------------|------------------------------------|
| Level 5: Premium | Pay to use the application. This either happens up-front, or after a trial period. |
| Level 4: Semi-premium | Use a limited number of features for free. Unlock the app with all features through an in-app purchase. |
| Level 3: In-app advertisement and in-app purchases | Free application with ads, encouraging users to remove ads or to make in-app purchases. |
| Level 2: Sample and premium | Two different versions of the same app. One is a version with limit features and/or ads. The other version is a premium version. |
| Level 1: In-app advertisement | Only one version of the app, with only ads and no in-app purchases. |
| Level 0: Free | The app has no monetization. However, money can still be made through selling user information. |

: Six levels of monetization for apps {#tbl-levels}

<!-- Ik heb dit expres kort gehouden, aangezien we ook nog een uitgebreidere literatuur review hebben. Maar, misschien moet dit toch nog iets langer? -Noa -->

## Societal Background

<!-- Is there any tie to practical contexts? How is this relevant to current business practices? -->

Currently, most apps utilize the freemium revenue model [@salehudin2021]. However, as discussed in @djaruma2023, there are many revenue models between completely premium and completely free. A more fine-grained classification of app revenue models beyond the traditional "paid-freemium-free" framework holds significant societal and business implications.

For society, such distinctions enhance transparency. Some monetization models, such as free or ad-filled apps, may rely on selling user information as a source of revenue [@bamberger2020]. Therefore, clearer distinctions regarding the revenue model will empower consumers to make informed choices. It may also enable policymakers to identify and regulate exploitative practices, such as manipulative microtransactions or intrusive ad models, ensuring all applications align with ethical and legal standards [@mileros2024].

For businesses, this paper should unlock more insight into the effectiveness of different revenue streams. This will allow developers to tailor monetization strategies to specific audiences. Furthermore, both consumers and regulatory bodies are growing more concerned with the privacy concerns of apps, especially ones that rely on market information [@mileros2024]. A granular understanding helps businesses adapt, aligning profitability with sustainability and ethical considerations.

<!-- Ik ben niet helemaal blij met de flow van dit stuk, ik mis een mooie conclusie. Maar, ik kan niet bedenken :(. Dit moet later toegevoegd worden -Noa. -->

## Research Gap

<!-- Kort samengevat: Wat is de relevantie? -->

In short, apps play an increasingly important role in our techno-centric society. To improve the user experience and increase profits, consideration of revenue models is key. Despite the great depth of research on this topic, literature tends to be focussed on the three big categories of paid, freemium, and free. This lack of nuance prevents us from understanding the fine-grained details that may help improve future apps.

<!-- Wat is de research gap? En wat is dus de onderzoekvraag? -->

The levels of monetization as proposed by @djaruma2023 would allow for this nuance. However, their framework has never been used in an empirical setting, as the paper by @djaruma2023 was published only last year. Applying this framework to see how different revenue streams impact the popularity of an app may yield valuable insights into the preferences of consumers. Therefore, the question to answer within this paper will be: *How are the 5 different revenue models as proposed by @djaruma2023 correlated to the success of an app?*

<!-- Wederom: kort maar krachtig, of té kort? -Noa -->

# Theory and Hypotheses <!-- 1,000-1,200 words -->

```{=html}
<!-- 

Comprehensive coverage of relevant literature, effectively building towards the research question.
Development of up to three hypotheses, clearly grounded in the theoretical framework.

Content: Review relevant literature to build a strong theoretical framework. Develop up to three hypotheses that directly connect to the theory and research question.

Key Elements:
Comprehensive literature review.
Explanation of key theoretical concepts.
Development of hypotheses. -->
```

In this section, prior research into the topic of revenue streams and its correlation to success in apps will be discussed. As mentioned in the Introduction section, this paper will apply the 5 levels of revenue as proposed by @djaruma2023 to app data. The following section will contain a holistic overview of the existing research, as well as hypotheses that arise from this theoretical framework.

## Literature Review {#sec-literature-review}

To answer the question "*How are the 5 different revenue models as proposed by @djaruma2023 correlate to the success of an app?*", we must first define what constitutes to success. In this paper, success will be defined by a couple of factors: popularity, rating, and estimated revenue.

### Popularity {#sec-popularity}

The popularity of an app can be measured by the number of downloads. It is important to note the popularity of an app is complex, and is not solely dependent on the chosen revenue model. Other features, such as whether an app is featured on charts, whether it has frequent updates, and word-of-mouth awareness, will also impact the popularity of an app [@aydingokgoz2021]. However, despite these other variables, to versions of the same app will still have drastically different performances with different revenue streams [@liu2014].

H1a: Apps that allow the user to have free access to all features (level 0 and 1) will have the highest amount of downloads overall. However, the ratings may fluctuate, as quality can vary for free-to-access apps. <!-- Is citation hier nodig? -->

H1b: The apps with the most downloads will be level 1. Most social media platforms, which dominate our culture, tend to have this revenue stream [@djaruma2023].

H1c: For apps that utilize a sample and a premium version of the same app (level 2), the free versions of an app will have more downloads than their paid-for counterpart. Most, if not all, users will download the free version first, and then might upgrade. This means there should be a disparity between the number of downloads between the apps, as is also demonstrated by @liu2012freemium.

H1d: The most downloaded apps in the gaming category will likely fall under level 4. Many popular games use this type of "pay-to-win" mechanism [@nieborg2016]. Therefore, it would be expected this same pattern would arise from our data.

### Rating {#sec-rating}

The downloads of an app are not everything. An app can be downloaded often, but may not be highly rated.

H2a: Apps that require the user to pay to unlock features (level 2, 3, and 4) will tend to have lower ratings than the version that requires payment upfront (level 5). The main draw of a freemium model is to attract users, and have them update to a paid version [@kumar2014]. However, as @kumar2014 points out, this can be a double-edged sword. Too few features, and it may not be attractive to users. Too many features, and the users will not update.

H2b: Fully premium apps (level 5) will have less variance in their ratings, while all other levels will have more. In the same vein as H2a, users have more realistic expectations of paid apps compared to apps that require you to unlock features [@kumar2014]. Therefore, more users downloading premium apps will be satisfied with their purchase, leading to less variance.

H3: For apps that utilize a sample and a premium version of the same app (level 2), the rating of the paid-for version is positively associated with the rating of the free version of the same app. This was true for the study on the most popular apps in the Google Play Store by @liu2012freemium, so it is expected a similar pattern should arise for this dataset.

### Revenue Estimation

<!-- Dit stuk kan later ook nog verschoven worden naar de discussie als dat beter is voor de flow. -Noa -->

It is important to point out downloads and ratings likely do not directly correlate to the actual revenue of an app. The revenue of apps "premium" apps that require an upfront payment, the revenue is relatively simple to track and compare. However, for apps that rely on advertisement, in-app purchases and/or selling market information, this is harder to track.

For apps that solely on advertisement, time retention can be a good measure of revenue [@ross2018]. However, this only works if the app solely relies on ads. An example of this given by @djaruma2023 is TikTok: this app relies not only on advertisement, but also on users purchasing products through its shop. Therefore, using solely the time retention would not accurately capture the revenue of an app with both revenue streams. Furthermore, the selling of user data is usually not publicized, meaning it is not possible to know the revenue from this.

Unfortunately, our data only contains the price of "premium" app versions. The data does not include any details regarding in-app purchases nor time-retention. Because of this lack of sufficient data, solely downloads and ratings will be taken into account as indicators of success.

<!-- Officieel mag je dus maar 3 hypothesen, maar ik heb ze onderverdeeld. Zou dit mogen? -Noa -->

# Methods and Data <!-- 1,000-1,200 words -->

```{=html}
<!--

Detailed description of the dataset, clear explanation of variable translation.
Proper use of statistical methods, with careful consideration of assumptions and appropriate handling of violations.

Content: Describe the dataset(s) used, explain variable selection and translation, and provide a detailed explanation of the statistical methods applied. Justify the methodological approach and handle assumptions rigorously.
Key Elements:
Dataset description.
Explanation of variable selection.
Statistical methods and justification.
Handling of assumptions.
-->
```

In this section, we will discuss the dataset and methods used to test the hypotheses outlined in the previous section. The focus lies on providing a comprehensive description of the dataset, including its structure and the variables it contains, followed by an explanation of the variable selection process. Additionally, we outline the statistical methods applied and discuss how assumptions, such as missing values and potential biases, were addressed to ensure the robustness of our analysis.

## Dataset Description

<!--Omschrijf de data: hoeveel instances, welke variables, waar gaat de data over?-->

The dataset used for this research consists of 1,016,666 instances and 27 variables, representing a detailed overview of mobile applications across various revenue models. Each instance corresponds to an app, and the variables capture key attributes such as app downloads, user ratings, and monetization strategies. Below is an overview of some of variables:

| Variable | Description |
|------------------------------------|------------------------------------|
| my_app_id (object) | Unique identifier for each app. |
| date_published (object) | The publication date of the app. Only three missing values (0.000295% null). |
| privacy_policy (object) | Information about the app's privacy policy, missing in 28.57% of cases. |
| rating_app (float64) | The average rating of the app, with 8.76% missing values. |
| nb_rating (object) | Number of ratings received by the app, missing in 8.76% of cases. |
| num_downloads (object) | The number of downloads for the app, nearly complete with only 15 missing values (0.001475% null). |
| price_gplay (object) | The price of the app as listed on Google Play, missing in 0.43% of cases. |
| in_app (bool) | Indicates whether the app has in-app purchases (no missing values). |
| has_ads (bool) | Indicates whether the app contains advertisements (no missing values). |
| content_rating_app (object) | The app's content rating, with three missing values (0.000295% null). |
| developer_name (object) | The name of the app developer, missing in only 16 instances (0.001574% null). |

The dataset includes several additional variables related to app features, developer information, and user engagement metrics such as visit_website, more_from_developer, and family_library. However, some variables, such as whats_new (100% null) and in_app_product (89.57% null), were deemed unsuitable for analysis due to their high proportion of missing data.

The primary purpose of this dataset in this study, is to analyze app monetization strategies by categorizing apps into distinct revenue levels and evaluating their performance based on key metrics like downloads and user ratings.

## Variable Selection

```{=html}
<!-- Welke variabelen gebruiken wij? Welke hebben we eruit gehaald? (Hierover uitbreiden in Handeling of Assumptions).

Hoe definiëren we de 5 levels?-->
```

The dataset utilized in this study consists of 1,016,666 entries, encompassing a broad range of attributes related to mobile applications. For the purpose of our analysis, 13 variables were selected, capturing critical information about app characteristics, user engagement, monetization strategies, and developer details. These variables include the app's unique identifier (my_app_id), the total number of downloads (num_downloads), average user ratings (rating_app), and the number of ratings (nb_rating). Additionally, the dataset provides information on app pricing (price_gplay), the presence of in-app purchases (in_app), and whether the app includes advertisements (has_ads). Other variables, such as content ratings (content_rating_app), app categories (categ_app), and developer information (developer_name and developer_info), further enhance the richness of the dataset. This subset of variables allows us to comprehensively examine the interplay between monetization strategies and app success.

To systematically explore monetization strategies, we classified the apps into six distinct levels based on their monetization models. These levels reflect varying approaches to generating revenue, ranging from completely free apps to fully premium paid apps.

Level 0 represents apps with no monetization, offering free services without ads or in-app purchases. At the opposite end, Level 5 includes premium apps requiring upfront payment, free from ads or in-app purchases, delivering a premium experience.

In between, Level 1 consists of free apps monetized solely through ads, while Level 3 combines ads and in-app purchases, offering additional features for users willing to pay. Level 4 refines the freemium model by removing ads and relying entirely on in-app purchases to monetize.

Level 2 employs a dual-version strategy, featuring both free sample apps with limited functionality (and potentially ads) and paid premium apps with comprehensive features and no ads or in-app purchases.

This classification is grounded in theoretical frameworks, such as the monetization levels proposed by Djaruma et al. (2023) and the App business models of (CITE), and allows for a nuanced analysis of how different revenue models impact app success metrics like user ratings and downloads. Our systematic categorization facilitates a deeper understanding of the relationship between monetization strategies and app performance.

## Statistical Methods

To test our hypotheses, we employed a combination of descriptive statistics, text processing, and machine learning techniques. Descriptive statistics were utilized to analyze distributions and trends in metrics such as num_downloads, rating_app, and price_gplay. We categorized applications into six monetization levels based on binary indicators: is_free, in_app, and has_ads. Price values were processed to distinguish between free and paid applications.

To identify paired sample and premium applications within level 2, we applied Term Frequency-Inverse Document Frequency (TF-IDF) vectorization combined with cosine similarity on application names. This approach is effective for measuring textual similarity between documents (CITE Source 2). Subsequent filtering involved prefix matching and the identification of indicative terms (e.g., "Free," "Pro") to ensure logical pairing based on naming conventions and shared developers.

To adjust ratings for applications with few reviews, we calculated a Bayesian average. This method provides a more robust measure of user satisfaction by accounting for the number of ratings and the overall average rating across all applications (CITE Source 3). Visualizations, including scatter plots and box plots, were employed to explore relationships between monetization levels and user engagement metrics which will be displayed in the results section.

### Handeling of Assumptions

We addressed missing values by removing rows with critical nulls, such as those in num_downloads, to maintain data integrity. Text-based variables like content_rating_app were standardized to ensure consistency. For price_gplay, currency symbols were removed to facilitate the classification of applications into free or paid categories.

Outliers in metrics like num_downloads were retained if they represented industry-leading applications, as their exclusion could skew the analysis. The use of Bayesian averages mitigated bias in rating_app due to low review counts, providing a more accurate reflection of user satisfaction. Covariance checks were conducted to ensure the absence of multicollinearity among numerical variables, thereby enhancing the reliability of correlation and regression analyses.

Some applications exhibited rare combinations of is_free, in_app, and has_ads that did not fit within the predefined monetization levels. These applications were excluded from the analysis but documented as a limitation. Edge cases in level 2 application pairing were flagged for potential mismatches due to naming ambiguities, ensuring transparency in the classification process.

These methodologies facilitated a systematic and accurate exploration of monetization models and their impact on application performance.

# Results <!-- 800-1,000 words -->

```{=html}
<!-- Clear presentation of results, including descriptive statistics and relevant tables/visualizations.
Effective interpretation of results, linked to hypotheses and research question.

Content: Present the results clearly, including descriptive statistics, tables, and visualizations. Interpret the results and link them back to the hypotheses and research question.

Key Elements:
Clear presentation of results.
Descriptive statistics.
Visualizations (e.g., graphs, charts).
Interpretation of results.

!! Voor volle punten, zorg dat je terug refereert naar de hypothesen !!
-->
```

In this section, we will visualize the data through tables and visualizations. These plots largely explore the data around the hypotheses and research question, we discussed in the previous sections. The aim of this section is to present possible evidence in supporting a hypothesis. This will be discussed and concluded upon in the next section.

Before we look at the results, let's revisit the research question: "*How are the 6 different revenue models as proposed by @djaruma2023 correlate to the success of an app?*". Where we identify 6 different revenue models, described in this @sec-literature-review as (revenue) levels. For reference see @tbl-levels.

The dataset is divided into these different levels. More than 75% of all the apps belong to level 0 and 1 (see @fig-distribution-amount-apps-levels). With the smallest population being level 2 with two different version of the same app.

```{python}
#| echo: false
#| warning: false
#| error: false
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from IPython.display import Markdown
from tabulate import tabulate

df_level_combined = pd.read_csv('./data/google_data_levels_combined.csv', dtype={16: 'string'})
df_mapping_level_2 = pd.read_csv('./data/mapping_level_2.csv', dtype={16: 'string'})
```

```{python}
#| echo: false
#| warning: false
#| error: false
#| label: fig-distribution-amount-apps-levels
#| fig-cap: "Distribution of the amount of apps across the revenue levels"

# Assuming df_level_combined['level'] exists
level_counts = df_level_combined['level'].value_counts()
level_counts = level_counts.reindex(["0", "1", "2 (Sample)", "2 (Premium)", "3", "4", "5"])  # Specify the order

# Creating labels for the legend with counts and percentages
total = level_counts.sum()
labels = [f"{level} - {count} ({count / total:.1%})" for level, count in level_counts.items()]

# Plotting the pie chart
plt.figure(figsize=(5, 5))
wedges, texts = plt.pie(
    level_counts, 
    labels=None,  # Hide labels on the pie chart itself
    startangle=90, 
    colors=plt.cm.Paired.colors
)

# Adding the custom legend
plt.legend(wedges, labels, title="Levels", loc="center left", bbox_to_anchor=(1, 0, 0.5, 1))

plt.show()


```

To find the success of an app, as discussed in @sec-literature-review. Metrics like popularity, the rating and the revenue estimation of an application can be used. The popularity measured by the number of downloads and rating by the average rating given by users. (Note: Revenue estimation is excluded, because this is not measurable with the data.) These metrics are used to evaluate the hypotheses (see @sec-literature-review).

## Number of Downloads. {#sec-number-of-downloads}

@fig-total-average-downloads-by-level provides insights into the number of app downloads categorized by revenue levels. @fig-total-average-downloads-by-level-1 displays the total amount of downloads, while the @fig-total-average-downloads-by-level-2 displays the average amount of downloads. In the next following three subsections, we will look into the key takeaways from these two graphs:

-   Distribution of app downloads: how the downloads are distributed among the revenue levels.

-   Gaming apps: analyzing the importance of gaming apps.

-   Free vs Paid: comparing free and paid apps.

```{python}
#| label: fig-total-average-downloads-by-level
#| fig-cap: "Comparison of Total and Average Downloads by Level"
#| fig-subcap:
#|   - "Total Number of Downloads by Level"
#|   - "Average Number of Downloads by Level" 
#| layout-ncol: 2
#| echo: false
#| warning: false
#| error: false

df_h1b = df_level_combined.copy()

# Calculate the sum of downloads for each level
total_downloads_h1b = df_h1b.groupby('level')['num_downloads'].sum().reset_index()

# Define the order of the levels
level_order = ["0", "1", "2 (Sample)", "2 (Premium)", "3", "4", "5"]

# First bar chart: Total Number of Downloads
sns.barplot(
    data=total_downloads_h1b,
    x='level',
    y='num_downloads',
    hue='level',
    palette='viridis',
    dodge=False,
    order=level_order
)
plt.xlabel('Level')
plt.ylabel('Total Number of Downloads')
plt.xticks(rotation=90)  # Rotate x-axis labels
plt.tight_layout()  # Adjust layout to avoid overlap
plt.show()

# Second bar chart: Average Number of Downloads
sns.barplot(
    data=df_h1b,
    x='level',
    y='num_downloads',
    hue='level',
    palette='viridis',
    dodge=False,
    order=level_order
)
plt.xlabel('Level')
plt.ylabel('Average Number of Downloads')
plt.xticks(rotation=90)  # Rotate x-axis labels
plt.tight_layout()  # Adjust layout to avoid overlap
plt.show()

```

### Distribution of app downloads. {#sec-distribution-app-downloads}

There is a significant difference between the total and average number of downloads for level 0 and 1, in @fig-total-average-downloads-by-level-1 and @fig-total-average-downloads-by-level-2 respectively. This is largely a result of the fact that 75% of the total apps fall under these levels (see @fig-distribution-amount-apps-levels.) By testing the hypothesis 1b, in @sec-popularity, it claims that apps in level 1 are downloaded the most, like social media platforms that dominate our culture. At a first glance the claim that this level "dominates our culture" doesn't appear to be the case.

Level 0 and 1 seem to have the highest overall downloads, as shown in @fig-total-average-downloads-by-level-1. @fig-downloads-free-access illustrates just how big the difference between the total downloads is. With a difference of over 40 billion downloads (0.4e11).

```{python}
#| label: fig-downloads-free-access
#| fig-cap: "Difference Total Number of Downloads"
#| echo: false
#| warning: false
#| error: false

df_h1a = df_level_combined.copy()

# if level is 0 or 1 then new column category is 'Free apps'
df_h1a['category'] = ['Free all feature apps' if level in ['0', '1'] else 'Free/Paid not all feature apps' for level in df_h1a['level']]

# Calculate the sum of downloads for each category
total_downloads_h1a = df_h1a.groupby('category')['num_downloads'].sum().reset_index()


# Make a bar chart 
plt.figure(figsize=(10, 6))
sns.barplot(data=total_downloads_h1a, x='category', y='num_downloads', hue='category', palette='viridis', dodge=False)
plt.xlabel('Category')
plt.ylabel('Total Number of Downloads')
plt.show()
```

@fig-distribution-app-downloads provides insights into distribution inequality. The distribution of the app downloads are highly concentrated, meaning a small proportion of the apps make up for the vast majority of the downloads. @fig-distribution-app-downloads-1 shows relatively the same inequality as @fig-distribution-app-downloads-2. Which indicates that a small amount of apps dominate the numbers, however this might also be present in other levels.

```{python}
#| label: fig-distribution-app-downloads
#| fig-cap: "Distribution of App Downloads (Top-Heavy Analysis)"
#| fig-subcap:
#|   - "All levels except 1"
#|   - "Level 1 Only" 
#| layout-ncol: 2
#| echo: false
#| warning: false
#| error: false

df_h1b = df_level_combined.copy()

# convert float to int for num_downloads
df_h1b['num_downloads'] = df_h1b['num_downloads'].astype(int)

# relevant columns
relevant_columns_h1b = ['my_app_id', 'num_downloads', 'categ_app', 'level', 'developer_name']

# filter the relevant columns
df_h1b = df_h1b[relevant_columns_h1b]

# Filter the dataframe to exclude level '1'
df_no_level_1 = df_h1b[df_h1b['level'] != '1'].copy()

# Sort the dataframe by 'num_downloads' in descending order
df_no_level_1_sorted = df_no_level_1.sort_values(by='num_downloads', ascending=False)

# Calculate the cumulative percentage of downloads
df_no_level_1_sorted['cumulative_percentage_downloads'] = (
    df_no_level_1_sorted['num_downloads'].cumsum() / df_no_level_1_sorted['num_downloads'].sum() * 100
)

# Create a Lorenz curve-style plot for all levels except '1'
plt.plot(
    range(1, len(df_no_level_1_sorted) + 1),
    df_no_level_1_sorted['cumulative_percentage_downloads'],
    label='Cumulative Downloads (All Levels Except 1)'
)
plt.plot(
    [1, len(df_no_level_1_sorted)],
    [0, 100],
    linestyle='--',
    color='gray',
    label='Equality Line'
)

# Add labels and legend
plt.xlabel('Apps (sorted by downloads)')
plt.ylabel('Cumulative Percentage of Downloads')
plt.legend()
plt.grid(True)
plt.show()


# Filter the dataframe for level '1'
df_level_1 = df_h1b[df_h1b['level'] == '1'].copy()

# Sort the dataframe by 'num_downloads' in descending order
df_level_1_sorted = df_level_1.sort_values(by='num_downloads', ascending=False)

# Calculate the cumulative percentage of downloads
df_level_1_sorted['cumulative_percentage_downloads'] = df_level_1_sorted['num_downloads'].cumsum() / df_level_1_sorted['num_downloads'].sum() * 100

# Create a Lorenz curve-style plot for level '1'
plt.plot(
    range(1, len(df_level_1_sorted) + 1),
    df_level_1_sorted['cumulative_percentage_downloads'],
    label='Cumulative Downloads (Level 1)'
)
plt.plot(
    [1, len(df_level_1_sorted)],
    [0, 100],
    linestyle='--',
    color='gray',
    label='Equality Line'
)

# Add labels and legend
plt.xlabel('Apps (sorted by downloads)')
plt.ylabel('Cumulative Percentage of Downloads')
plt.legend()
plt.grid(True)
plt.show()
```

The download range of apps in @fig-num-downloads-by-level-log illustrates that level 1 dominates in terms of apps with over 1 billion+ downloads. Furthermore, level 0 also has a few apps with over 1 billion downloads.

```{python}
#| label: fig-num-downloads-by-level-log
#| fig-cap: "Number of Downloads by Level (Log Scale)"
#| echo: false
#| warning: false
#| error: false

downloads_across_level = df_level_combined.copy()

# Define the bins and labels
bins = [0, 100, 1000, 10000, 100000, 1000000, 10000000, 100000000, 1000000000, np.inf]
labels = ['0-100', '101-1k', '1k-10k', '10k-100k', '100k-1M', '1M-10M', '10M-100M', '100M-1B', '1B+']

# Create a new column 'downloads_bin' based on the number of downloads
downloads_across_level['downloads_bin'] = pd.cut(downloads_across_level['num_downloads'], bins=bins, labels=labels)

# Group by 'level' and 'downloads_bin' to get the count of apps per download bin for each level
grouped = downloads_across_level.groupby(['level', 'downloads_bin']).size().unstack(fill_value=0)

# Plotting the bar chart with log scale
grouped.plot(kind='bar', stacked=False, figsize=(12, 8), logy=True)
plt.xlabel('Level')
plt.ylabel('Number of Apps (Log Scale)')
plt.xticks(rotation=90)
plt.legend(title='Download Range', bbox_to_anchor=(1.05, 1), loc='upper left')
plt.tight_layout()
plt.show()


```

The apps categories Communication and Social classify as social platforms as shown in @fig-num-downloads-1-billion. Attributing to 40% of all the apps with more than 1 billion downloads.

```{python}
#| label: fig-num-downloads-1-billion
#| fig-cap: "1 Billion+ Downloads in Level 1"
#| echo: false
#| warning: false
#| error: false

downloads_across_level = df_level_combined.copy()
downloads_across_level = downloads_across_level[downloads_across_level['level'] == '1'].copy()

# Add '1B Downloads' column
downloads_across_level['1B Downloads'] = downloads_across_level['num_downloads'] >= 1_000_000_000

# Filter for apps with 1 billion or more downloads
billion_downloads = downloads_across_level[downloads_across_level['1B Downloads'] == True]

# Preparing data
billion_downloads_summary = billion_downloads['categ_app'].value_counts().reset_index()
billion_downloads_summary.columns = ['Category', 'Number of Apps']

# Sort and place "Social" next to "Communication"
custom_order = ['Communication', 'Social'] + [
    cat for cat in billion_downloads_summary['Category'] if cat not in ['Communication', 'Social']
]
billion_downloads_summary = billion_downloads_summary.set_index('Category').loc[custom_order].reset_index()

# Data for the pie chart
sizes = billion_downloads_summary['Number of Apps']
labels = billion_downloads_summary['Category']

# Plotting the pie chart
plt.figure(figsize=(6, 6))  # Adjust the size of the pie chart
wedges, texts, autotexts = plt.pie(
    sizes, 
    labels=None,  # Hide labels on the pie chart itself
    autopct='%1.1f%%',  # Display percentages
    startangle=90, 
    colors=plt.cm.Paired(np.linspace(0, 1, len(labels)))  # Distinct colors
)

# Customize percentage text size and color
for autotext in autotexts:
    autotext.set_color('white')
    autotext.set_fontsize(10)

# Adding the custom legend
plt.legend(
    wedges, 
    labels, 
    title="Categories", 
    loc="center left", 
    bbox_to_anchor=(1, 0, 0.5, 1)  # Custom legend position
)

# Display the chart
plt.tight_layout()
plt.show()
```

Taking a closer look at the apps provided by [@djaruma2023] in @tbl-top-8-apps-paper. We do see that Facebook, Instagram, Spotify, Snapchat and Amazon Shopping fall under level 1. (Disclaimer: TikTok didn't exist up until 2019).

```{python}
#| label: tbl-top-8-apps-paper
#| tbl-cap: "Top 8 Apps by Downloads (in Billions)"
#| echo: false
#| warning: false
#| error: false


df_h1b = df_h1b[df_h1b['my_app_id'].isin(['com.amazon.mShop.android.shopping', 'com.instagram.android', 'com.facebook.lite', 'com.netflix.mediaclient', 'com.snapchat.android', 'com.spotify.music', 'com.whatsapp', 'com.facebook.orca'])]

df_h1b = df_h1b[['my_app_id', 'num_downloads', 'categ_app', 'level']]

df_h1b['num_downloads'] = df_h1b['num_downloads'] / 1_000_000_000

app_mapping = {
    'com.amazon.mShop.android.shopping': 'Amazon Shopping',
    'com.instagram.android': 'Instagram',
    'com.facebook.lite': 'Facebook Lite',
    'com.netflix.mediaclient': 'Netflix',
    'com.snapchat.android': 'Snapchat',
    'com.spotify.music': 'Spotify',
    'com.whatsapp': 'WhatsApp',
    'com.facebook.orca': 'Facebook Messenger'
}

# Map app names to the 'my_app_id' column
df_h1b['app_name'] = df_h1b['my_app_id'].map(app_mapping)

# Reorder columns for better readability
df_h1b = df_h1b[['app_name', 'num_downloads', 'categ_app', 'level']]


# Rename columns as per requirement
df_h1b = df_h1b.rename(columns={
    'app_name': 'App Name',
    'num_downloads': 'Downloads (in B)',
    'level': 'Level',
    'categ_app': 'Category'
})

df_h1b.sort_values(by='Downloads (in B)', ascending=False, inplace=True)
df_h1b.reset_index(drop=True, inplace=True)
Markdown(tabulate(df_h1b, headers=df_h1b.columns))
```

It can be concluded that the most downloaded apps fall under level 1 (see @fig-total-average-downloads-by-level-1). But this requires a nuanced view, since 50% of the apps fall under level 1 (see @fig-distribution-amount-apps-levels) and it has a relatively low average among other levels (see @fig-total-average-downloads-by-level-2). So, it requires further analysis to understand the popularity of level 1. As seen from @fig-distribution-app-downloads it shows that there is high inequality, resulting in a small proportion of the apps accounting to a majority of the downloads. Furthermore, @fig-num-downloads-by-level-log shows that apps in level 1 are considered among the top downloaded apps. In particular, the categories under which social media platform fall (see @fig-num-downloads-1-billion). Looking at the apps provided by @djaruma2023, most social network platforms indeed fall under level 1 (see @tbl-top-8-apps-paper). Given these observations, it can be concluded that there is enough evidence to support h1b (see @sec-popularity).

### Games in the Google Play Store {#sec-gaming-apps}

The difference between total and average number of downloads in level 3 and 4 is not relatively significant as shown in @fig-total-average-downloads-by-level. Furthermore, it is relatively high in both instances. Hypothesis 1d, which takes a closer look into these 2 revenue levels, claims that many popular games fall under these 2.

According to @fig-proportion-games-total-downloads-2, 15.5% of the total apps are games. While these games attribute to 26.7% of the total app downloads, as shown in @fig-proportion-games-total-downloads-1.

```{python}
#| label: fig-proportion-games-total-downloads
#| fig-cap: "Proportion of Games compared to other apps"
#| fig-subcap:
#|   - "Total Downloads: Gaming vs Other Apps"
#|   - "Total Amount: Gaming vs Other Apps"
#| layout-ncol: 2
#| echo: false
#| warning: false
#| error: false

all_apps = df_level_combined.copy()

gaming_categories = [
    "Puzzle", "Casual", "Arcade", "Simulation", "Action", "Adventure", 
    "Trivia", "Racing", "Educational", "Card", "Word", "Board", 
    "Casino", "Role Playing", "Strategy", "Brain Games", 
    "Action & Adventure", "Pretend Play"
]

# Filter other than gaming apps
other_apps = all_apps[~all_apps['categ_app'].isin(gaming_categories)]


df_level_combined_h1d = df_level_combined.copy()

# relevant columns
relevant_columns_h1d = ['my_app_id', 'num_downloads', 'level', 'categ_app']

# filter the relevant columns
df_level_combined_h1d = df_level_combined_h1d[relevant_columns_h1d]

# Creating a list of categories that can be considered as part of the gaming category
gaming_categories = [
    "Puzzle", "Casual", "Arcade", "Simulation", "Action", "Adventure", 
    "Trivia", "Racing", "Educational", "Card", "Word", "Board", 
    "Casino", "Role Playing", "Strategy", "Brain Games", 
    "Action & Adventure", "Pretend Play"
]

# Filter on games category
df_level_combined_h1d = df_level_combined_h1d[df_level_combined_h1d['categ_app'].isin(gaming_categories)]

gaming_apps = df_level_combined_h1d.copy()

# relevant columns
relevant_columns_other_apps = ['my_app_id', 'num_downloads', 'level', 'categ_app']

# filter the relevant columns
other_apps = other_apps[relevant_columns_other_apps]

# Calculate the sum of downloads for each category
total_downloads_gaming_apps = gaming_apps['num_downloads'].sum()
total_downloads_other_apps = other_apps['num_downloads'].sum()
avg_downloads_gaming_apps = gaming_apps['num_downloads'].mean()
avg_downloads_other_apps = other_apps['num_downloads'].mean()
total_amount_gaming_apps = len(gaming_apps)
total_amount_other_apps = len(other_apps)

# Creating a new dataframe to represent the totals
summary_data = {
    "Category": ["Gaming Apps", "Other Apps"],
    "Total Downloads": [total_downloads_gaming_apps, total_downloads_other_apps],
    "Total Amount": [total_amount_gaming_apps, total_amount_other_apps],
    "Average Downloads": [avg_downloads_gaming_apps, avg_downloads_other_apps]

}
df_summary = pd.DataFrame(summary_data)

# Pie chart for Total Downloads
wedges1, texts1, autotexts1 = plt.pie(
    df_summary['Total Downloads'], labels=df_summary['Category'], autopct='%1.1f%%', startangle=140
)
plt.legend(wedges1, df_summary['Category'], title="Categories", loc="upper right", bbox_to_anchor=(1.3, 0.9))
plt.tight_layout()
plt.show()

# Pie chart for Total Amount
wedges2, texts2, autotexts2 = plt.pie(
    df_summary['Total Amount'], labels=df_summary['Category'], autopct='%1.1f%%', startangle=140
)
plt.legend(wedges2, df_summary['Category'], title="Categories", loc="upper right", bbox_to_anchor=(1.3, 0.9))
plt.tight_layout()
plt.show()
```

The average downloads of games is 98.6% higher than the average downloads of the other apps (see @tbl-avg-downloads-games).

```{python}
#| label: tbl-avg-downloads-games
#| tbl-cap: "Average Downloads Games vs Other Apps"
#| echo: false
#| warning: false
#| error: false

# Combine both datasets
all_apps = pd.concat([gaming_apps, other_apps])

# Calculate averages
gaming_avg = gaming_apps['num_downloads'].mean()
other_avg = other_apps['num_downloads'].mean()
overall_avg = all_apps['num_downloads'].mean()

# Percentage difference between gaming apps and overall average
gaming_vs_baseline_difference = ((gaming_avg - overall_avg) / overall_avg) * 100

# Percentage difference between other apps and overall average
other_vs_baseline_difference = ((other_avg - overall_avg) / overall_avg) * 100

# Percentage difference between gaming apps and other apps
gaming_vs_other_difference = ((gaming_avg - other_avg) / other_avg) * 100

# Create a summary table with shorter column names
summary_table = pd.DataFrame({
    'Category': ['Gaming', 'Other', 'All'],
    'Avg Downloads': [gaming_avg, other_avg, overall_avg],
    '% Diff Gaming vs Other': [
        gaming_vs_other_difference.round(2), 
        (gaming_vs_other_difference * -1).round(2), 
        "-"  # No comparison for the baseline
    ]
})

# Round the values for better readability
summary_table = summary_table.round(2)
Markdown(tabulate(summary_table, headers=summary_table.columns))
```

Up until now, these findings conclude that games are hugely popular across all levels. @fig-avg-downloads-level-3-4 shows that games, especially in level 3 and 4, are even more popular. Averaging around 1 million downloads, compared to close to 0.5 million in @tbl-avg-downloads-games.

```{python}
#| label: fig-avg-downloads-level-3-4
#| fig-cap: "Average amount of Downloads for Gaming Apps by Level"
#| echo: false
#| warning: false
#| error: false

df_level_combined_h1d = df_level_combined.copy()

# relevant columns
relevant_columns_h1d = ['my_app_id', 'num_downloads', 'level', 'categ_app']

# filter the relevant columns
df_level_combined_h1d = df_level_combined_h1d[relevant_columns_h1d]

# Creating a list of categories that can be considered as part of the gaming category
gaming_categories = [
    "Puzzle", "Casual", "Arcade", "Simulation", "Action", "Adventure", 
    "Trivia", "Racing", "Educational", "Card", "Word", "Board", 
    "Casino", "Role Playing", "Strategy", "Brain Games", 
    "Action & Adventure", "Pretend Play"
]

# Filter on games category
df_level_combined_h1d = df_level_combined_h1d[df_level_combined_h1d['categ_app'].isin(gaming_categories)]

# barplot the number of downloads for each level
sns.barplot(data=df_level_combined_h1d, x='level', y='num_downloads', palette='viridis', order=level_order)
plt.xlabel('Level')
plt.ylabel('Number of Downloads')
plt.xticks(rotation=90)  # Rotate x-axis labels
plt.show()
```

The variance of level 4 is higher than level 3 as shown in @tbl-avg-downloads-games. In @tbl-top-5-level-3-4-games, it reveals a notable variance in downloads between the 2 revenue levels. The variance in level 4 is much larger than in level 3, even if it only displays the top 5. For instance, the downloads in level 4 varies from 0.5 to 0.1 billion. While the downloads in level 3 range from 1.0 to 0.5 billion.

```{python}
#| label: tbl-top-5-level-3-4-games
#| tbl-cap: "Combined top 5 games of level 3 and 4"
#| echo: false
#| warning: false
#| error: false

# Filter for level 3, sort by downloads, and pick top 5
top_apps_level_3 = df_level_combined_h1d[df_level_combined_h1d['level'] == '3'] \
    .sort_values(by='num_downloads', ascending=False) \
    .head(5)

app_name_mapping = {
    "com.kiloo.subwaysurf": "Subway Surfers",
    "com.fingersoft.hillclimb": "Hill Climb Racing",
    "com.imangi.templerun2": "Temple Run 2",
    "com.outfit7.mytalkingtomfree": "My Talking Tom",
    "me.pou.app": "Pou"
}

# Convert num_downloads to per billion
top_apps_level_3['num_downloads'] = top_apps_level_3['num_downloads'] / 1_000_000_000

# Mapping the app column to their normal names
top_apps_level_3['App Name'] = top_apps_level_3['my_app_id'].map(app_name_mapping)

# Rename columns as per requirement
top_apps_level_3 = top_apps_level_3.rename(columns={
    'num_downloads': 'Downloads (in Billions)',
    'categ_app': 'Gaming Category'
})

top_apps_level_3.reset_index(drop=True, inplace=True)
top_apps_level_3 = top_apps_level_3[['App Name', 'Downloads (in Billions)', 'level']]

# Filter for level 4, sort by downloads, and pick top 5
top_apps_level_4 = df_level_combined_h1d[df_level_combined_h1d['level'] == '4'] \
    .sort_values(by='num_downloads', ascending=False) \
    .head(5)

additional_app_name_mapping = {
    "com.king.candycrushsaga": "Candy Crush Saga",
    "com.supercell.clashofclans": "Clash of Clans",
    "com.king.petrescuesaga": "Pet Rescue Saga",
    "com.king.farmheroessaga": "Farm Heroes Saga",
    "com.king.candycrushsodasaga": "Candy Crush Soda Saga"
}

# Convert num_downloads to per billion
top_apps_level_4['num_downloads'] = top_apps_level_4['num_downloads'] / 1_000_000_000

# Mapping the app column to their normal names
top_apps_level_4['App Name'] = top_apps_level_4['my_app_id'].map(additional_app_name_mapping)

# Rename columns as per requirement
top_apps_level_4 = top_apps_level_4.rename(columns={
    'num_downloads': 'Downloads (in Billions)',
    'categ_app': 'Gaming Category'
})

top_apps_level_4.reset_index(drop=True, inplace=True)
top_apps_level_4 = top_apps_level_4[['App Name', 'Downloads (in Billions)','level']]

# Combine the tables
combined_tables = pd.concat([top_apps_level_3, top_apps_level_4])
combined_tables.sort_values(by='Downloads (in Billions)', ascending=False, inplace=True)
combined_tables.reset_index(drop=True, inplace=True)
Markdown(tabulate(combined_tables, headers=combined_tables.columns))
```

The games in the Google Play Store play a significant role. As shown in @fig-proportion-games-total-downloads-1, it attributes to 26.7% of the total downloads. Furthermore, the average downloads of games is 98.6% more than any other apps with an average close to 0.5 million downloads (see @tbl-avg-downloads-games). In level 3 and 4, the average downloads jumps to over 1 million average downloads (see @fig-avg-downloads-level-3-4). Given these observations, it can be concluded that there is enough evidence to support h1d (see @sec-popularity).

### Sample and premium Apps {#sec-sample-premium-apps}

Levels 2 (premium) and 5 have the least amount of total and average downloads, as shown in @fig-total-average-downloads-by-level. Furthermore, we also see a high disparity in average downloads between level 2 sample and premium, as shown in @fig-total-average-downloads-by-level-2.

As shown in @fig-proportion-sample-premium , just 1.1% of the downloads are attributed to premium apps within all apps in level 2. Illustrating in depth just how high this offset is.

```{python}
#| label: fig-proportion-sample-premium
#| fig-cap: "Proportion of Total Downloads: Sample vs Premium"
#| echo: false
#| warning: false
#| error: false

df_sample_h1c = df_level_combined[df_level_combined['level'] == '2 (Sample)']
df_premium_h1c = df_level_combined[df_level_combined['level'] == '2 (Premium)']
df_mapping_h1c = df_mapping_level_2.copy()

# relevant columns
relevant_columns_h1c = ['my_app_id', 'num_downloads', 'level', 'rating_app']
relevant_columns_mapping_h1c =  ['Sample app name',	'Premium app name']

# filter the relevant columns
df_sample_h1c = df_sample_h1c[relevant_columns_h1c]
df_premium_h1c = df_premium_h1c[relevant_columns_h1c]
df_mapping_h1c = df_mapping_h1c[relevant_columns_mapping_h1c]

# Merge mapping dataframe with sample dataframe and premium dataframe
df_mapped_h1c = (
    df_mapping_h1c
    .merge(df_sample_h1c[['my_app_id', 'num_downloads', 'rating_app']], left_on='Sample app name', right_on='my_app_id', how='left')
    .drop(columns=['my_app_id'])
    .rename(columns={'num_downloads': 'Sample Downloads', 'rating_app': 'Sample Rating'})
    .merge(df_premium_h1c[['my_app_id', 'num_downloads', 'rating_app']], left_on='Premium app name', right_on='my_app_id', how='left')
    .drop(columns=['my_app_id'])
    .rename(columns={'num_downloads': 'Premium Downloads', 'rating_app': 'Premium Rating'})
)

# Calculate metrics
df_mapped_h1c['Download Difference'] = df_mapped_h1c['Premium Downloads'] - df_mapped_h1c['Sample Downloads']
df_mapped_h1c['Download Ratio'] = df_mapped_h1c['Premium Downloads'] / df_mapped_h1c['Sample Downloads']

# Calculating total sample and premium downloads
total_sample_downloads = df_mapped_h1c['Sample Downloads'].sum()
total_premium_downloads = df_mapped_h1c['Premium Downloads'].sum()

# Creating a new dataframe to represent the totals
summary_data_h1c = {
    "Version Type": ["Sample Downloads", "Premium Downloads"],
    "Total Downloads": [total_sample_downloads, total_premium_downloads]
}
df_summary_h1c = pd.DataFrame(summary_data_h1c)

# Creating a pie chart to show the proportion of sample vs premium downloads
fig, ax = plt.subplots(figsize=(5, 5))
ax.pie(df_summary_h1c['Total Downloads'], labels=df_summary_h1c['Version Type'], autopct='%1.1f%%', startangle=140)

plt.show()
```

High disparity is also evident in the average downloads, as shown in @tbl-premium-sample-diff. Where the average download difference is close to 500.000. With an average download ratio of nearly 27%. Meaning that about one in fourth users that download the sample app, also download the premium app.

```{python}
#| label: tbl-premium-sample-diff
#| tbl-cap: "Sample and Premium download metrics"
#| echo: false
#| warning: false
#| error: false

df_sample_h1c = df_level_combined[df_level_combined['level'] == '2 (Sample)']
df_premium_h1c = df_level_combined[df_level_combined['level'] == '2 (Premium)']
df_mapping_h1c = df_mapping_level_2.copy()

# relevant columns
relevant_columns_h1c = ['my_app_id', 'num_downloads', 'level', 'rating_app']
relevant_columns_mapping_h1c =  ['Sample app name',	'Premium app name']

# filter the relevant columns
df_sample_h1c = df_sample_h1c[relevant_columns_h1c]
df_premium_h1c = df_premium_h1c[relevant_columns_h1c]
df_mapping_h1c = df_mapping_h1c[relevant_columns_mapping_h1c]

# Merge mapping dataframe with sample dataframe and premium dataframe
df_mapped_h1c = (
    df_mapping_h1c
    .merge(df_sample_h1c[['my_app_id', 'num_downloads', 'rating_app']], left_on='Sample app name', right_on='my_app_id', how='left')
    .drop(columns=['my_app_id'])
    .rename(columns={'num_downloads': 'Sample Downloads', 'rating_app': 'Sample Rating'})
    .merge(df_premium_h1c[['my_app_id', 'num_downloads', 'rating_app']], left_on='Premium app name', right_on='my_app_id', how='left')
    .drop(columns=['my_app_id'])
    .rename(columns={'num_downloads': 'Premium Downloads', 'rating_app': 'Premium Rating'})
)

# Calculate metrics
df_mapped_h1c['Download Difference'] = df_mapped_h1c['Premium Downloads'] - df_mapped_h1c['Sample Downloads']
df_mapped_h1c['Download Ratio'] = df_mapped_h1c['Premium Downloads'] / df_mapped_h1c['Sample Downloads']


# Average downloads and ratios
avg_sample_downloads = df_mapped_h1c['Sample Downloads'].mean()
avg_premium_downloads = df_mapped_h1c['Premium Downloads'].mean()
avg_download_difference = df_mapped_h1c['Download Difference'].mean()
avg_download_ratio = df_mapped_h1c['Download Ratio'].mean()

# Data for the new dataframe
summary_data_h1c = {
    "Metric": [
        "Average Sample Downloads",
        "Average Premium Downloads",
        "Average Download Difference",
        "Average Download Ratio"
    ],
    "Value": [
        avg_sample_downloads,
        avg_premium_downloads,
        avg_download_difference,
        avg_download_ratio
    ]
}

# Create the summary dataframe
df_summary_h1c = pd.DataFrame(summary_data_h1c)
Markdown(tabulate(df_summary_h1c, headers=df_summary_h1c.columns))
```

Hypothesis 1c claims that the sample version indeed has more downloads than their paid-for counterpart. This claim is supported by examining @fig-total-average-downloads-by-level and @fig-total-average-downloads-by-level-2. However, for a more detailed perspective like the proportion of the sample vs premium downloads (see @fig-proportion-sample-premium), this further strengthens this argument. Additionally, the high disparity across metrics in @tbl-premium-sample-diff is even more evidence to support h1c (see @sec-popularity).

## Ratings

The average and Bayesian average (adjusting for total reviews) are relatively the same across all revenue levels, as shown in @fig-proportion-rating.

```{python}
#| label: fig-proportion-rating
#| fig-cap: "Proportion of Ratings Across Levels"
#| echo: false
#| warning: false
#| error: false

ratings_across_level = df_level_combined.copy()
# Group by 'level' and calculate mean for 'rating_app' and 'bayesian_average'
average_ratings = ratings_across_level.groupby('level')['rating_app'].mean()
bayesian_ratings = ratings_across_level.groupby('level')['bayesian_average'].mean()
# Plotting the ratings across levels
plt.figure(figsize=(10, 6))

# Bar plot for ratings and Bayesian ratings
average_ratings.plot(kind='bar', width=0.4, position=1, label='Average Rating', color='blue', alpha=0.6)
bayesian_ratings.plot(kind='bar', width=0.4, position=0, label='Bayesian Rating', color='green', alpha=0.6)

# Adding labels and title
plt.xlabel('Level')
plt.ylabel('Rating')
plt.legend()
plt.xticks(rotation=0)

# Show plot
plt.tight_layout()
plt.show()
```

The variance of the ratings across the different levels is where we do see lot of variance, as illustrated in @fig-variance-rating. However, when adjusting for total reviews: the Bayesian average rating, the variance diminishes. This is logical, since it smooths out the fluctuations and provides a more consistent view of the ratings. This shows that levels especially the paid apps (level 2 premium and 5) have strong outliers.

```{python}
#| label: fig-variance-rating
#| fig-cap: "Variance of Different Attributes Grouped by Level"
#| echo: false
#| warning: false
#| error: false

variance_rating = df_level_combined.copy()

# Assuming your dataframe is named df
columns_to_calculate = ['rating_app', 'bayesian_average']
variance_specific = variance_rating.groupby('level')[columns_to_calculate].var().reset_index()

# Melting the dataframe to long format for easier plotting
variance_melted = variance_specific.melt(id_vars='level', var_name='Attribute', value_name='Variance')

plt.figure(figsize=(12, 6))
sns.barplot(data=variance_melted, x='level', y='Variance', hue='Attribute', palette='viridis')
plt.xlabel('Level')
plt.ylabel('Variance')
plt.xticks(rotation=45)
plt.legend(title='Attribute', loc='upper right')
plt.tight_layout()
plt.show()
```

In the next two subsections we investigate further on the variance and the difference in rating between paid and free apps.

In the next following three subsections, we will investigate further into:

-   Quality of Free and Paid Apps: Fluctuations in ratings caused by the quality.

-   Variance in Ratings: Less fluctuation in premium apps than their free counterpart.

-   Correlation between Sample and Premium Apps: Correlation in terms of rating between two app versions.

### Quality of Free and Paid Apps {#sec-quality-free-paid-apps}

Level 0 and 1 have the highest overall downloads, as shown in @fig-downloads-free-access by quite a margin (40 billion downloads (0.4e11)) compared to the other apps. These apps under level 0 and 1 have free access to all features. Hypothesis 1a (see @sec-popularity) suggests that the variance in rating between "free access to all feature apps" and "free access to not all features or paid apps" is a sign of quality disparity.

@tbl-free-access shows the variance and standard deviation between "free access to all feature apps" and "free access to not all features or paid apps". From which we can conclude that there is not a big statistical difference. Roughly, 0.02 and 0.01 difference in variance and standard deviation rating respectively.

```{python}
#| label: tbl-free-access
#| tbl-cap: "Variance (and std) difference. Between free access to all feature apps and free access to not all features or paid apps"
#| echo: false
#| warning: false
#| error: false


df_h1a = df_level_combined.copy()

# if level is 0 or 1 then new column category is 'Free apps'
df_h1a['category'] = ['Free all feature apps' if level in ['0', '1'] else 'Free/Paid not all feature apps' for level in df_h1a['level']]

# Calculate variance and standard deviation for each category
df_stats_h1a = df_h1a.groupby('category')['rating_app'].agg(['var', 'std']).reset_index()

# Rename columns for clarity
df_stats_h1a.rename(columns={'var': 'variance in rating', 'std': 'standard deviation in rating'}, inplace=True)

Markdown(tabulate(df_stats_h1a, headers=df_stats_h1a.columns))
```

@fig-free-access visualizes variance with outliers. It shows that both categories have a similar median rating close to 4.5, with "Free/Paid not all feature apps" being slightly lower. Both categories seem to have a lot of outliers under 3, with 'free all feature apps' having more outliers in 2 and 1.

```{python}
#| label: fig-free-access
#| fig-cap: "Boxplot of Ratings by App Category"
#| echo: false
#| warning: false
#| error: false

# Step 1: Add 'category' column based on level
df_h1a = df_level_combined.copy()
df_h1a['category'] = [
    'Free all feature apps' if level in ['0', '1'] else 'Free/Paid not all feature apps' 
    for level in df_h1a['level']
]

# Step 3: Create a boxplot for ratings categorized by 'category'

# Prepare data for custom boxplot
categories = df_h1a['category'].unique()
ratings_data = [df_h1a[df_h1a['category'] == category]['rating_app'] for category in categories]

# Custom boxplot
plt.figure(figsize=(10, 6))
plt.boxplot(ratings_data, labels=categories, patch_artist=True)
plt.ylabel('Ratings')
plt.xlabel('App Category')
plt.grid(axis='y', linestyle='--', alpha=0.7)

plt.show()
```

The variance and standard deviation is minimal between the two categories, as shown in @tbl-free-access . @fig-free-access illustrates that both categories receive high ratings, with significant number of outliers. These outliers can be interpreted as variability in user satisfaction. The slighty higher median in 'Free all feature apps' may imply that users prefer this over 'Free/Paid not all feature apps'. However, this is marginal and would probably require statistical validation. Thus inconclusive as evidence to support h1a (see @sec-popularity).

Conversely, hypothesis 2a (see @sec-rating) suggests that the apps where users need to pay to unlock features tend to have a lower rating than apps that require payment upfront.

As shown in @fig-free-vs-paid, premium apps have a higher rating on average compared to free apps with users needing to pay to unlock features. The free apps show a greater spread in ratings.

```{python}
#| label: fig-free-vs-paid
#| fig-cap: "Boxplot of Ratings for Free and Premium Apps"
#| echo: false
#| warning: false
#| error: false

df_level_combined_h2a = df_level_combined.copy()

# Filter only levels 2, 3, 4, and 5
df_h2a = df_level_combined_h2a[df_level_combined_h2a['level'].isin(['2 (Sample)', '2 (Premium)', '3', '4', '5'])].copy()

# Filter on premium and free apps
df_h2a['category'] = np.where((df_h2a['level'] == '5') | (df_h2a['level'] == '2 (Premium)'), 'Premium apps', 'Free apps')

# relevant columns
relevant_columns_h2a = ['category', 'rating_app', 'bayesian_average']

# filter the relevant columns
df_h2a = df_h2a[relevant_columns_h2a]

# Calculating the mean rating for Free and Premium apps
mean_rating_free = df_h2a[df_h2a['category'] == 'Free apps']['rating_app'].mean()
mean_rating_premium = df_h2a[df_h2a['category'] == 'Premium apps']['rating_app'].mean()

# Calculating the mean Bayesian average for Free and Premium apps
mean_bayesian_free = df_h2a[df_h2a['category'] == 'Free apps']['bayesian_average'].mean()
mean_bayesian_premium = df_h2a[df_h2a['category'] == 'Premium apps']['bayesian_average'].mean()

# Creating a new DataFrame from these results
mean_results = pd.DataFrame({
    'Category': ['Free apps', 'Premium apps'],
    'Mean Rating': [mean_rating_free, mean_rating_premium],
    'Mean Bayesian Average': [mean_bayesian_free, mean_bayesian_premium]
})

# Combine ratings for boxplot
categories = ['Free apps', 'Premium apps']
ratings_data = [df_h2a[df_h2a['category'] == cat]['rating_app'] for cat in categories]

plt.boxplot(ratings_data, labels=categories, patch_artist=True)
plt.ylabel('Ratings')
plt.xlabel('App Category')
plt.grid(axis='y', linestyle='--', alpha=0.7)

plt.show()
```

The apps that require upfront payment receive a higher rating on average then free apps, shown in @fig-free-vs-paid. This can indicate that users perceive as being of higher quality. The greater spread of ratings for free apps, may indicate that the app quality varies more. This suffices as enough evidence to support h2a (see @sec-rating).

### Variance in Ratings

@fig-var-rating-other-premium illustrates a significant difference in the variance of ratings. Premium apps tend to have a higher variance, with more outliers.

```{python}
#| label: fig-var-rating-other-premium
#| fig-cap: "Variance in Ratings for Other vs Premium Apps"
#| echo: false
#| warning: false
#| error: false


df_level_combined_h2b = df_level_combined.copy()

# Filter on level 5 and others
df_level_combined_h2b['category'] = np.where(df_level_combined_h2b['level'].isin(['2 (Premium)', '5']), 'Premium apps', 'Other apps')

# relevant columns
relevant_columns_h2b = ['category', 'rating_app', 'bayesian_average']

# filter the relevant columns
df_level_combined_h2b = df_level_combined_h2b[relevant_columns_h2b]

# Filtering data for Free and Premium apps
other_apps = df_level_combined_h2b[df_level_combined_h2b['category'] == 'Other apps']
premium_apps = df_level_combined_h2b[df_level_combined_h2b['category'] == 'Premium apps']

# Calculating variances for ratings and bayesian averages
other_variance_rating = other_apps['rating_app'].var()
premium_variance_rating = premium_apps['rating_app'].var()

# Creating a bar plot for variances in ratings
fig, ax = plt.subplots(figsize=(10, 6))
ax.bar(['Other apps', 'Premium apps'], [other_variance_rating, premium_variance_rating], color=['skyblue', 'orange'])
ax.set_ylabel('Variance in Ratings', fontsize=12)
ax.set_ylim(0, max(other_variance_rating, premium_variance_rating) * 1.2)

plt.tight_layout()
plt.show()
```

@fig-var-bayesian-other-premium illustrates the variance of the ratings and bayesian variance. For level 5 we see that the variance is one of the lowest when adjusted by Bayesian average. This difference may arise because users who are satisfied with the app tend to rate the app highly, while users who are not satisfied, tend to rate it much lower, having paid for the app.

```{python}
#| label: fig-var-bayesian-other-premium
#| fig-cap: "Variance of Attributes Across Different Levels"
#| echo: false
#| warning: false
#| error: false

variance_rating = df_level_combined.copy()

# Assuming your dataframe is named df
columns_to_calculate = ['rating_app', 'bayesian_average']
variance_specific = variance_rating.groupby('level')[columns_to_calculate].var().reset_index()

# Melting the dataframe to long format for easier plotting
variance_melted = variance_specific.melt(id_vars='level', var_name='Attribute', value_name='Variance')

# Melting the dataframe to long format
variance_melted = variance_specific.melt(id_vars='level', var_name='Attribute', value_name='Variance')

# Plotting line plot with seaborn
sns.lineplot(data=variance_melted, x='level', y='Variance', hue='Attribute', marker='o', palette='Set1')
plt.xlabel('Level')
plt.ylabel('Variance')
plt.xticks(rotation=45)
plt.legend(title='Attribute')
plt.tight_layout()
plt.show()
```

In conclusion, the variance across all levels show that level 5 has the highest variance for rating and the lowest variance for bayesian rating, as shown in @fig-var-bayesian-other-premium. The bayesian average seems more reliable regarding the overall app performance. The premium apps may have polarized feedback due to high user expectations. In that case it's enough evidence to support h2b (see @sec-rating).

### Correlation between Sample and Premium Apps {#sec-correlation}

Revenue level 2 has two versions of the same app. A sample and a premium version.

@tbl-correlation shows that the correlation between ratings of sample and premium apps is considered moderate positive with 0.35. The correlation between the Bayesian average rating of sample and premium apps is considered moderate to strong positive correlation with 0.5.

```{python}
#| label: tbl-correlation
#| tbl-cap: "Correlation ranking between sample and premium"
#| echo: false
#| warning: false
#| error: false

df_sample_h3 = df_level_combined[df_level_combined['level'] == '2 (Sample)']
df_premium_h3 = df_level_combined[df_level_combined['level'] == '2 (Premium)']
df_mapping_h3 = df_mapping_level_2.copy()

# relevant columns
relevant_columns_h3 = ['my_app_id', 'level', 'rating_app', 'bayesian_average']
relevant_columns_mapping_h3 =  ['Sample app name',	'Premium app name']

# filter the relevant columns
df_sample_h3 = df_sample_h3[relevant_columns_h3]
df_premium_h3 = df_premium_h3[relevant_columns_h3]
df_mapping_h3 = df_mapping_h3[relevant_columns_mapping_h3]

# Merge mapping dataframe with sample dataframe and premium dataframe
df_mapped_h3 = (
    df_mapping_h3
    .merge(df_sample_h3[['my_app_id', 'bayesian_average', 'rating_app']], left_on='Sample app name', right_on='my_app_id', how='left')
    .drop(columns=['my_app_id'])
    .rename(columns={'num_downloads': 'Sample Downloads', 'rating_app': 'Sample Rating', 'bayesian_average': 'Sample Bayesian Average'})
    .merge(df_premium_h3[['my_app_id', 'bayesian_average', 'rating_app']], left_on='Premium app name', right_on='my_app_id', how='left')
    .drop(columns=['my_app_id'])
    .rename(columns={'num_downloads': 'Premium Downloads', 'rating_app': 'Premium Rating', 'bayesian_average': 'Premium Bayesian Average'})
)

# Correlation Coefficient
correlation_rating_h3 = df_mapped_h3["Sample Rating"].corr(df_mapped_h3["Premium Rating"])

# Correlation Coefficient for Bayesian averages
correlation_bayesian_rating_h3 = df_mapped_h3["Sample Bayesian Average"].corr(df_mapped_h3["Premium Bayesian Average"])

correlation_data_h3 = {
    "Metric": ["Sample Rating vs Premium Rating", "Sample Bayesian Average vs Premium Bayesian Average"],
    "Correlation Coefficient": [correlation_rating_h3, correlation_bayesian_rating_h3]
}
correlation_data_h3= pd.DataFrame(correlation_data_h3)

Markdown(tabulate(correlation_data_h3, headers=correlation_data_h3.columns))
```

@tbl-correlation shows moderate to strong positive correlation when using the bayesian average rating. This provides evidence to support H2c (see @sec-rating).

## Conclusion

| Hypothesis | Support                         | Sufficient evidence? |
|------------|---------------------------------|----------------------|
| H1a        | @sec-quality-free-paid-apps     | Inconclusive         |
| H1b        | @sec-distribution-app-downloads | Yes                  |
| H1c        | @sec-sample-premium-apps        | Yes                  |
| H1d        | @sec-gaming-apps                | Yes                  |
| H2a        | @sec-quality-free-paid-apps     | Yes                  |
| H2b        | @sec-rating                     | No                   |
| H2c        | @sec-correlation                | Yes                  |

The research question: "*How are the 6 different revenue models as proposed by @djaruma2023 correlate to the success of an app?*". This success was measured by popularity and rating.

HOE MOETEN WE DE RESEARCH QUESTION BEANTWOORDEN???

# Discussion <!--500-700 words -->

```{=html}
<!-- Insightful discussion of findings in relation to the research question and literature.
Reflection on practical implications and contributions to the ongoing debate.

Content: Provide a thoughtful discussion of the findings in relation to the research question and the literature. Reflect on the practical implications and contributions to the academic debate.

Key Elements:
Reflection on findings.
Linkage to existing literature.
Practical
-->
```

## Reflection on the Findings

Downloads do not necessarily indicate revenue for freemium models [@djaruma2023]. The time the user spends on an app and the purchases made within this app [@ross2018] are better measures of the revenue for freemium applications.

## Practical Implications for Businesses

## Future Research Directions

# References

::: {#refs}
:::
